{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-analysis-nb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BvFgORPfaaY",
        "outputId": "a20e6ab4-63ad-4375-9685-a746049bcf35"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import stopwords, twitter_samples"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CjJqzStgvAj"
      },
      "source": [
        "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZDMIaJohIKj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_pos = pos_tweets[:4000]\n",
        "test_pos = pos_tweets[4000:]\n",
        "train_neg = neg_tweets[:4000]\n",
        "test_neg = neg_tweets[4000:]\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbNbLgu2hdtu"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "            word not in string.punctuation):  # remove punctuation\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tIUyUs_ht3C",
        "outputId": "438b189b-5d34-435c-d414-34eb273e61f0"
      },
      "source": [
        "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
        "\n",
        "# print cleaned tweet\n",
        "print(process_tweet(custom_tweet))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VIIzxcxoLKv",
        "outputId": "db44ebc9-7e51-4617-8f59-b2b4488c01cf"
      },
      "source": [
        "train_x[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
              " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
              " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
              " '@97sides CONGRATS :)',\n",
              " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days',\n",
              " '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM',\n",
              " \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\",\n",
              " '@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.',\n",
              " 'Jgh , but we have to go to Bayan :D bye',\n",
              " 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell… as the name implies :p.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zrXxTNWoSKr"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(tokenizer=process_tweet)\n",
        "train_x_vecs = vectorizer.fit_transform(train_x).toarray()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERYid368_sXK",
        "outputId": "80afecba-b243-4dae-ec47-865af2cc49db"
      },
      "source": [
        "train_x_vecs.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 9084)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itOEU9_j_tz8",
        "outputId": "3f561605-8154-426a-cb7e-034c2de258c0"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB()\n",
        "model.fit(train_x_vecs, train_y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0_IQMFDAJA0"
      },
      "source": [
        "test_x_vecs = vectorizer.transform(test_x)\n",
        "preds = model.predict(test_x_vecs)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IZI0HC_AUkS",
        "outputId": "903f8a9b-bb96-4fc4-c4bd-c508a93e862c"
      },
      "source": [
        "preds"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azTtMnKaAbnA",
        "outputId": "3000c4bd-2c82-4d36-9350-df1b0b7d2acc"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "print(f'accuracy={accuracy_score(test_y, preds)}')\n",
        "print(f'f1={f1_score(test_y, preds)}')\n",
        "print(f'confusion={confusion_matrix(test_y, preds)}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy=0.5905\n",
            "f1=0.4095169430425379\n",
            "confusion=[[897 103]\n",
            " [716 284]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqOzdGhxA34z",
        "outputId": "c39b47f6-3272-45b2-bde7-58a432581d9f"
      },
      "source": [
        "train_x_vecs.toarray()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKjdYaAqICBu"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.num_classes = 0\n",
        "        self.num_features = 0\n",
        "        self.log_prior = None\n",
        "        self.log_likelihood = None\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.num_classes = len(np.unique(y))\n",
        "        self.num_features = X.shape[1]\n",
        "        self.log_prior = np.zeros(self.num_classes)\n",
        "        self.log_likelihood = np.zeros((self.num_features, self.num_classes))\n",
        "\n",
        "        for c in range(self.num_classes):\n",
        "            self.log_prior[c] = np.log(len(y[y == c]) / len(y))\n",
        "        \n",
        "        for f in range(self.num_features):\n",
        "            for c in range(self.num_classes):\n",
        "                sub_x = X[y == c]\n",
        "                self.log_likelihood[f, c] = np.log((np.sum(sub_x[:, f]) + 1) / (np.sum(sub_x)) + self.num_features)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = np.dot(X, self.log_likelihood) + self.log_prior.reshape((1, -1))\n",
        "        return np.argmax(probs, axis=1)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQx4-E1aQ8ix",
        "outputId": "fc910d87-c613-4495-ff6b-d59d397ad2ad"
      },
      "source": [
        "model = NaiveBayes()\n",
        "model.fit(train_x_vecs, train_y)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.NaiveBayes at 0x7f5d8f796fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjsh5yW6SPeK"
      },
      "source": [
        "test_x_vecs = vectorizer.transform(test_x)\n",
        "predictions = model.predict(test_x_vecs.toarray())"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrDJ_vJkUq1M",
        "outputId": "b26fff23-43a2-4d37-e5e1-e88b90307ae2"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "print(f'accuracy={accuracy_score(test_y, predictions)}')\n",
        "print(f'f1={f1_score(test_y, predictions)}')\n",
        "print(f'confusion={confusion_matrix(test_y, predictions)}')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy=0.9935\n",
            "f1=0.993483709273183\n",
            "confusion=[[996   4]\n",
            " [  9 991]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGpeftDIa0H7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}